{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMw9sE5mHxr7",
        "colab_type": "text"
      },
      "source": [
        "# Shubham Shrama\n",
        "## IIT BOMBAY\n",
        "In this repository,  we'll impliment U-Net network on the cell dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlzcHZ76hKkO",
        "colab_type": "code",
        "outputId": "3d86e729-89c2-4f4a-9ae7-f3bede05081f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#As my data is on my drive in Cell_data folder, we'll have to mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnUQnGQ7g670",
        "colab_type": "code",
        "outputId": "a0264089-6be6-4966-d19b-2dc6380db769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from keras.optimizers import RMSprop,SGD, adam\n",
        " \n",
        "from keras import callbacks \n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#from GeneralizationPOC.NetPixel import TransferLearning_19_Heart,TransferLearning_19_Test_rbc,U_Net\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "np.random.seed(1337) # for reproducibility\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "#from FirstPackage.Network import net_1\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
        "set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfpQV1qj7li",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "width=512\n",
        "height=512\n",
        "depth=3\n",
        "classes=2\n",
        "weightsPath=None\n",
        "keras.backend.set_image_data_format('channels_first')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi5_X96ZhW3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def U_Net(width, height, depth, classes,weightsPath=None):\n",
        "        \n",
        "    bn_flag=True\n",
        "    data_shape = height*width\n",
        "    \n",
        "    inputs = Input((depth,width, height))\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization(axis = 1)(conv1,training=bn_flag)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization(axis=1)(conv1,training=bn_flag)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization(axis=1)(conv2,training=bn_flag)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization(axis=1)(conv2,training=bn_flag)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization(axis=1)(conv3,training=bn_flag)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization(axis=1)(conv3,training=bn_flag)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    \n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization(axis=1)(conv4,training=bn_flag)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization(axis=1)(conv4,training=bn_flag)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    \n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    \n",
        "    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=1)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    \n",
        "    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=1)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    \n",
        "    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=1)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    \n",
        "    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=1)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    \n",
        "    conv10 = Conv2D(classes, (3, 3), padding='same')(conv9)\n",
        "             \n",
        "    reshape = Reshape((classes, data_shape), input_shape=(classes, height, width))(conv10)\n",
        "    perm = Permute((2, 1))(reshape)\n",
        "    softmax = Activation(\"softmax\")(perm)\n",
        "    \n",
        "    model = Model(inputs=[inputs], outputs=[softmax])\n",
        "   \n",
        "    if weightsPath is not None:\n",
        "        model.load_weights(weightsPath)\n",
        "    \n",
        "    print(model.summary())\n",
        "    return model \n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYhaly1-dc9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pathtoload='gdrive/My Drive/Cell_data/'\n",
        "#Loading training and testing sets\n",
        "x_train=np.load(pathtoload+'x_train.npy')\n",
        "y_train=np.load(pathtoload+'y_train.npy')\n",
        "x_test=np.load(pathtoload+'x_test.npy')\n",
        "y_test=np.load(pathtoload+'y_test.npy')\n",
        "x_train_whitened=np.load(pathtoload+'x_train_whitened.npy')\n",
        "x_test_whitened=np.load(pathtoload+'x_test_whitened.npy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGdcfR1ZZRME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path2save = './Cell.h5'\n",
        "\n",
        "#Define Image Size and Number of classes\n",
        "imheight = 512\n",
        "imwidth = 512\n",
        "imdepth = 3\n",
        "data_shape = imheight*imwidth\n",
        "classes = 2\n",
        "\n",
        "# #Load numpy arrays from the directory\n",
        "# train_data = np.load(train_data_path+'data_cell.npy')\n",
        "# # train_data = np.transpose(train_data,[0,2,3,1])\n",
        "# #train_data = train_data.astype(\"float32\")\n",
        "# label = np.reshape(label,(len(label),data_shape,classes))\n",
        "y_train = np.reshape(y_train,(len(y_train),data_shape,classes))\n",
        "y_test = np.reshape(y_test,(len(y_test),data_shape,classes))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feCFH89eefkM",
        "colab_type": "code",
        "outputId": "d69abd87-c362-4d7a-98b2-874dbdee6073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8778
        }
      },
      "source": [
        "#Create Network Graph in the Memory\n",
        "net = U_Net(imwidth, imheight, imdepth, classes,weightsPath='Cell.h5_0200-1.0000.h5')\n",
        "#print(net.summary())\n",
        "\n",
        "print (\"Compiling Model...\")\n",
        "#Store the network weights whenever loss is minimum than previous epoch\n",
        "modelCheck = callbacks.ModelCheckpoint(path2save+'_{epoch:04d}-{acc:.4f}.h5', monitor='acc', mode='auto')\n",
        "# modelCheck = callbacks.ModelCheckpoint(path2save, monitor='acc', verbose=0, save_best_only=True, mode='auto')\n",
        " \n",
        "opt = adam(lr=1e-4)\n",
        "#Set the compiler parameter for the training\n",
        "net.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"],sample_weight_mode='auto')\n",
        "print (\"Training the Model...\")\n",
        " \n",
        "#Train the Network\n",
        "net.fit(x_train_whitened, y_train, batch_size = 18, epochs= 500, verbose=1,callbacks= [modelCheck],validation_data=(x_test_whitened, y_test))\n",
        "print (\"Dumping Weights to file...\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_9 (InputLayer)            (None, 3, 512, 512)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 32, 512, 512) 896         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 32, 512, 512) 128         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 32, 512, 512) 9248        batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 32, 512, 512) 128         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_33 (MaxPooling2D) (None, 32, 256, 256) 0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 64, 256, 256) 18496       max_pooling2d_33[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 64, 256, 256) 256         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 64, 256, 256) 36928       batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 64, 256, 256) 256         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_34 (MaxPooling2D) (None, 64, 128, 128) 0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 128, 128, 128 73856       max_pooling2d_34[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 128, 128, 128 512         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 128, 128, 128 147584      batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 128, 128, 128 512         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling2D) (None, 128, 64, 64)  0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 256, 64, 64)  295168      max_pooling2d_35[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 256, 64, 64)  1024        conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 256, 64, 64)  590080      batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 256, 64, 64)  1024        conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling2D) (None, 256, 32, 32)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 512, 32, 32)  1180160     max_pooling2d_36[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 512, 32, 32)  2359808     conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_33 (Conv2DTran (None, 256, 64, 64)  524544      conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 512, 64, 64)  0           conv2d_transpose_33[0][0]        \n",
            "                                                                 batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 256, 64, 64)  1179904     concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 256, 64, 64)  590080      conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_34 (Conv2DTran (None, 128, 128, 128 131200      conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 256, 128, 128 0           conv2d_transpose_34[0][0]        \n",
            "                                                                 batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 128, 128, 128 295040      concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 128, 128, 128 147584      conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_35 (Conv2DTran (None, 64, 256, 256) 32832       conv2d_166[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 128, 256, 256 0           conv2d_transpose_35[0][0]        \n",
            "                                                                 batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_167 (Conv2D)             (None, 64, 256, 256) 73792       concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_168 (Conv2D)             (None, 64, 256, 256) 36928       conv2d_167[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_36 (Conv2DTran (None, 32, 512, 512) 8224        conv2d_168[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 64, 512, 512) 0           conv2d_transpose_36[0][0]        \n",
            "                                                                 batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_169 (Conv2D)             (None, 32, 512, 512) 18464       concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_170 (Conv2D)             (None, 32, 512, 512) 9248        conv2d_169[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_171 (Conv2D)             (None, 2, 512, 512)  578         conv2d_170[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_9 (Reshape)             (None, 2, 262144)    0           conv2d_171[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "permute_9 (Permute)             (None, 262144, 2)    0           reshape_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 262144, 2)    0           permute_9[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 7,764,482\n",
            "Trainable params: 7,762,562\n",
            "Non-trainable params: 1,920\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Compiling Model...\n",
            "Training the Model...\n",
            "Train on 536 samples, validate on 134 samples\n",
            "Epoch 1/500\n",
            "536/536 [==============================] - 90s 167ms/step - loss: 0.4627 - acc: 0.9691 - val_loss: 0.4197 - val_acc: 0.9707\n",
            "Epoch 2/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.2647 - acc: 0.9773 - val_loss: 0.2170 - val_acc: 0.9684\n",
            "Epoch 3/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0898 - acc: 0.9739 - val_loss: 0.0957 - val_acc: 0.9670\n",
            "Epoch 4/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0597 - acc: 0.9783 - val_loss: 0.0857 - val_acc: 0.9696\n",
            "Epoch 5/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0530 - acc: 0.9798 - val_loss: 0.0841 - val_acc: 0.9701\n",
            "Epoch 6/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0499 - acc: 0.9809 - val_loss: 0.0826 - val_acc: 0.9704\n",
            "Epoch 7/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0471 - acc: 0.9819 - val_loss: 0.0807 - val_acc: 0.9706\n",
            "Epoch 8/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0454 - acc: 0.9824 - val_loss: 0.0825 - val_acc: 0.9704\n",
            "Epoch 9/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0420 - acc: 0.9835 - val_loss: 0.0852 - val_acc: 0.9705\n",
            "Epoch 10/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0408 - acc: 0.9840 - val_loss: 0.0841 - val_acc: 0.9710\n",
            "Epoch 11/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0818 - val_acc: 0.9710\n",
            "Epoch 12/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0858 - val_acc: 0.9710\n",
            "Epoch 13/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0842 - val_acc: 0.9710\n",
            "Epoch 14/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0339 - acc: 0.9866 - val_loss: 0.0884 - val_acc: 0.9710\n",
            "Epoch 15/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0328 - acc: 0.9870 - val_loss: 0.0863 - val_acc: 0.9711\n",
            "Epoch 16/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0313 - acc: 0.9875 - val_loss: 0.0906 - val_acc: 0.9713\n",
            "Epoch 17/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0302 - acc: 0.9879 - val_loss: 0.0906 - val_acc: 0.9711\n",
            "Epoch 18/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0291 - acc: 0.9883 - val_loss: 0.0924 - val_acc: 0.9715\n",
            "Epoch 19/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0283 - acc: 0.9887 - val_loss: 0.0935 - val_acc: 0.9714\n",
            "Epoch 20/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0270 - acc: 0.9892 - val_loss: 0.0951 - val_acc: 0.9716\n",
            "Epoch 21/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0268 - acc: 0.9893 - val_loss: 0.0962 - val_acc: 0.9710\n",
            "Epoch 22/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0256 - acc: 0.9897 - val_loss: 0.1000 - val_acc: 0.9713\n",
            "Epoch 23/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0248 - acc: 0.9899 - val_loss: 0.0988 - val_acc: 0.9719\n",
            "Epoch 24/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0238 - acc: 0.9904 - val_loss: 0.1020 - val_acc: 0.9717\n",
            "Epoch 25/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0233 - acc: 0.9905 - val_loss: 0.1015 - val_acc: 0.9718\n",
            "Epoch 26/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0237 - acc: 0.9904 - val_loss: 0.1014 - val_acc: 0.9719\n",
            "Epoch 27/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0231 - acc: 0.9907 - val_loss: 0.1030 - val_acc: 0.9718\n",
            "Epoch 28/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0220 - acc: 0.9910 - val_loss: 0.1076 - val_acc: 0.9718\n",
            "Epoch 29/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0224 - acc: 0.9910 - val_loss: 0.1035 - val_acc: 0.9720\n",
            "Epoch 30/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0210 - acc: 0.9914 - val_loss: 0.1077 - val_acc: 0.9721\n",
            "Epoch 31/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0207 - acc: 0.9916 - val_loss: 0.1084 - val_acc: 0.9719\n",
            "Epoch 32/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0200 - acc: 0.9918 - val_loss: 0.1113 - val_acc: 0.9719\n",
            "Epoch 33/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0192 - acc: 0.9921 - val_loss: 0.1151 - val_acc: 0.9718\n",
            "Epoch 34/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0186 - acc: 0.9924 - val_loss: 0.1164 - val_acc: 0.9717\n",
            "Epoch 35/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0189 - acc: 0.9923 - val_loss: 0.1145 - val_acc: 0.9718\n",
            "Epoch 36/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0183 - acc: 0.9925 - val_loss: 0.1200 - val_acc: 0.9718\n",
            "Epoch 37/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0181 - acc: 0.9926 - val_loss: 0.1178 - val_acc: 0.9719\n",
            "Epoch 38/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0174 - acc: 0.9929 - val_loss: 0.1204 - val_acc: 0.9721\n",
            "Epoch 39/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0175 - acc: 0.9928 - val_loss: 0.1208 - val_acc: 0.9722\n",
            "Epoch 40/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0171 - acc: 0.9930 - val_loss: 0.1216 - val_acc: 0.9721\n",
            "Epoch 41/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0169 - acc: 0.9931 - val_loss: 0.1234 - val_acc: 0.9720\n",
            "Epoch 42/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0161 - acc: 0.9934 - val_loss: 0.1267 - val_acc: 0.9724\n",
            "Epoch 43/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0169 - acc: 0.9931 - val_loss: 0.1223 - val_acc: 0.9723\n",
            "Epoch 44/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0161 - acc: 0.9934 - val_loss: 0.1254 - val_acc: 0.9721\n",
            "Epoch 45/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0155 - acc: 0.9936 - val_loss: 0.1282 - val_acc: 0.9723\n",
            "Epoch 46/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0155 - acc: 0.9937 - val_loss: 0.1294 - val_acc: 0.9722\n",
            "Epoch 47/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0149 - acc: 0.9939 - val_loss: 0.1326 - val_acc: 0.9723\n",
            "Epoch 48/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0147 - acc: 0.9940 - val_loss: 0.1342 - val_acc: 0.9721\n",
            "Epoch 49/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0145 - acc: 0.9940 - val_loss: 0.1352 - val_acc: 0.9724\n",
            "Epoch 50/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0138 - acc: 0.9943 - val_loss: 0.1410 - val_acc: 0.9722\n",
            "Epoch 51/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0141 - acc: 0.9942 - val_loss: 0.1393 - val_acc: 0.9721\n",
            "Epoch 52/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0133 - acc: 0.9945 - val_loss: 0.1408 - val_acc: 0.9724\n",
            "Epoch 53/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0131 - acc: 0.9946 - val_loss: 0.1454 - val_acc: 0.9722\n",
            "Epoch 54/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0133 - acc: 0.9946 - val_loss: 0.1443 - val_acc: 0.9722\n",
            "Epoch 55/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0129 - acc: 0.9947 - val_loss: 0.1467 - val_acc: 0.9724\n",
            "Epoch 56/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0127 - acc: 0.9948 - val_loss: 0.1485 - val_acc: 0.9723\n",
            "Epoch 57/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0128 - acc: 0.9948 - val_loss: 0.1465 - val_acc: 0.9721\n",
            "Epoch 58/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0120 - acc: 0.9951 - val_loss: 0.1534 - val_acc: 0.9722\n",
            "Epoch 59/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0121 - acc: 0.9950 - val_loss: 0.1518 - val_acc: 0.9722\n",
            "Epoch 60/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0120 - acc: 0.9951 - val_loss: 0.1513 - val_acc: 0.9723\n",
            "Epoch 61/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0115 - acc: 0.9952 - val_loss: 0.1573 - val_acc: 0.9723\n",
            "Epoch 62/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0116 - acc: 0.9952 - val_loss: 0.1547 - val_acc: 0.9723\n",
            "Epoch 63/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0111 - acc: 0.9954 - val_loss: 0.1621 - val_acc: 0.9721\n",
            "Epoch 64/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0114 - acc: 0.9953 - val_loss: 0.1558 - val_acc: 0.9724\n",
            "Epoch 65/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0110 - acc: 0.9955 - val_loss: 0.1599 - val_acc: 0.9725\n",
            "Epoch 66/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0108 - acc: 0.9956 - val_loss: 0.1638 - val_acc: 0.9723\n",
            "Epoch 67/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0104 - acc: 0.9957 - val_loss: 0.1634 - val_acc: 0.9726\n",
            "Epoch 68/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.1677 - val_acc: 0.9725\n",
            "Epoch 69/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0104 - acc: 0.9957 - val_loss: 0.1642 - val_acc: 0.9727\n",
            "Epoch 70/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0101 - acc: 0.9958 - val_loss: 0.1663 - val_acc: 0.9725\n",
            "Epoch 71/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0100 - acc: 0.9959 - val_loss: 0.1674 - val_acc: 0.9724\n",
            "Epoch 72/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0099 - acc: 0.9959 - val_loss: 0.1689 - val_acc: 0.9725\n",
            "Epoch 73/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0095 - acc: 0.9961 - val_loss: 0.1714 - val_acc: 0.9725\n",
            "Epoch 74/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0092 - acc: 0.9962 - val_loss: 0.1775 - val_acc: 0.9725\n",
            "Epoch 75/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0092 - acc: 0.9962 - val_loss: 0.1761 - val_acc: 0.9724\n",
            "Epoch 76/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0091 - acc: 0.9963 - val_loss: 0.1760 - val_acc: 0.9725\n",
            "Epoch 77/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0090 - acc: 0.9963 - val_loss: 0.1760 - val_acc: 0.9726\n",
            "Epoch 78/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0088 - acc: 0.9964 - val_loss: 0.1760 - val_acc: 0.9727\n",
            "Epoch 79/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0087 - acc: 0.9964 - val_loss: 0.1799 - val_acc: 0.9726\n",
            "Epoch 80/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0085 - acc: 0.9965 - val_loss: 0.1835 - val_acc: 0.9725\n",
            "Epoch 81/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0084 - acc: 0.9965 - val_loss: 0.1848 - val_acc: 0.9725\n",
            "Epoch 82/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.1826 - val_acc: 0.9727\n",
            "Epoch 83/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 0.1860 - val_acc: 0.9725\n",
            "Epoch 84/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 0.1858 - val_acc: 0.9726\n",
            "Epoch 85/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0079 - acc: 0.9968 - val_loss: 0.1870 - val_acc: 0.9727\n",
            "Epoch 86/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0082 - acc: 0.9966 - val_loss: 0.1874 - val_acc: 0.9724\n",
            "Epoch 87/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0079 - acc: 0.9967 - val_loss: 0.1886 - val_acc: 0.9727\n",
            "Epoch 88/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0074 - acc: 0.9970 - val_loss: 0.1950 - val_acc: 0.9726\n",
            "Epoch 89/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 0.1937 - val_acc: 0.9726\n",
            "Epoch 90/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0074 - acc: 0.9969 - val_loss: 0.1933 - val_acc: 0.9726\n",
            "Epoch 91/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 0.1864 - val_acc: 0.9725\n",
            "Epoch 92/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0075 - acc: 0.9969 - val_loss: 0.1925 - val_acc: 0.9726\n",
            "Epoch 93/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0072 - acc: 0.9971 - val_loss: 0.1933 - val_acc: 0.9728\n",
            "Epoch 94/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0071 - acc: 0.9971 - val_loss: 0.1943 - val_acc: 0.9728\n",
            "Epoch 95/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.2018 - val_acc: 0.9726\n",
            "Epoch 96/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0068 - acc: 0.9972 - val_loss: 0.1995 - val_acc: 0.9726\n",
            "Epoch 97/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0070 - acc: 0.9971 - val_loss: 0.1939 - val_acc: 0.9729\n",
            "Epoch 98/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0065 - acc: 0.9973 - val_loss: 0.2027 - val_acc: 0.9729\n",
            "Epoch 99/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0066 - acc: 0.9973 - val_loss: 0.2039 - val_acc: 0.9726\n",
            "Epoch 100/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0066 - acc: 0.9973 - val_loss: 0.2043 - val_acc: 0.9726\n",
            "Epoch 101/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.1994 - val_acc: 0.9728\n",
            "Epoch 102/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0063 - acc: 0.9974 - val_loss: 0.2064 - val_acc: 0.9727\n",
            "Epoch 103/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.2064 - val_acc: 0.9727\n",
            "Epoch 104/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0063 - acc: 0.9974 - val_loss: 0.2081 - val_acc: 0.9726\n",
            "Epoch 105/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0061 - acc: 0.9975 - val_loss: 0.2078 - val_acc: 0.9727\n",
            "Epoch 106/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.2130 - val_acc: 0.9726\n",
            "Epoch 107/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0061 - acc: 0.9975 - val_loss: 0.2066 - val_acc: 0.9728\n",
            "Epoch 108/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.2135 - val_acc: 0.9727\n",
            "Epoch 109/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 0.2130 - val_acc: 0.9728\n",
            "Epoch 110/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.2113 - val_acc: 0.9727\n",
            "Epoch 111/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.2114 - val_acc: 0.9728\n",
            "Epoch 112/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0055 - acc: 0.9977 - val_loss: 0.2180 - val_acc: 0.9728\n",
            "Epoch 113/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0053 - acc: 0.9978 - val_loss: 0.2170 - val_acc: 0.9729\n",
            "Epoch 114/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.2224 - val_acc: 0.9728\n",
            "Epoch 115/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0054 - acc: 0.9978 - val_loss: 0.2175 - val_acc: 0.9729\n",
            "Epoch 116/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0054 - acc: 0.9978 - val_loss: 0.2206 - val_acc: 0.9727\n",
            "Epoch 117/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0053 - acc: 0.9978 - val_loss: 0.2229 - val_acc: 0.9726\n",
            "Epoch 118/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 0.2254 - val_acc: 0.9729\n",
            "Epoch 119/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 0.2277 - val_acc: 0.9729\n",
            "Epoch 120/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.2259 - val_acc: 0.9728\n",
            "Epoch 121/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 0.2283 - val_acc: 0.9728\n",
            "Epoch 122/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 0.2284 - val_acc: 0.9729\n",
            "Epoch 123/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0047 - acc: 0.9981 - val_loss: 0.2295 - val_acc: 0.9729\n",
            "Epoch 124/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.2230 - val_acc: 0.9727\n",
            "Epoch 125/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0050 - acc: 0.9979 - val_loss: 0.2281 - val_acc: 0.9728\n",
            "Epoch 126/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0047 - acc: 0.9981 - val_loss: 0.2307 - val_acc: 0.9728\n",
            "Epoch 127/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 0.2311 - val_acc: 0.9726\n",
            "Epoch 128/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.2332 - val_acc: 0.9727\n",
            "Epoch 129/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.2328 - val_acc: 0.9728\n",
            "Epoch 130/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 0.2277 - val_acc: 0.9730\n",
            "Epoch 131/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.2327 - val_acc: 0.9729\n",
            "Epoch 132/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.2359 - val_acc: 0.9728\n",
            "Epoch 133/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.2383 - val_acc: 0.9730\n",
            "Epoch 134/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 0.2353 - val_acc: 0.9728\n",
            "Epoch 135/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 0.2386 - val_acc: 0.9728\n",
            "Epoch 136/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.2398 - val_acc: 0.9728\n",
            "Epoch 137/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2409 - val_acc: 0.9729\n",
            "Epoch 138/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0045 - acc: 0.9982 - val_loss: 0.2310 - val_acc: 0.9730\n",
            "Epoch 139/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2405 - val_acc: 0.9729\n",
            "Epoch 140/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0044 - acc: 0.9982 - val_loss: 0.2356 - val_acc: 0.9728\n",
            "Epoch 141/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2413 - val_acc: 0.9729\n",
            "Epoch 142/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.2399 - val_acc: 0.9730\n",
            "Epoch 143/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2418 - val_acc: 0.9728\n",
            "Epoch 144/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2373 - val_acc: 0.9731\n",
            "Epoch 145/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2396 - val_acc: 0.9729\n",
            "Epoch 146/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2369 - val_acc: 0.9730\n",
            "Epoch 147/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.2409 - val_acc: 0.9730\n",
            "Epoch 148/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.2458 - val_acc: 0.9729\n",
            "Epoch 149/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.2454 - val_acc: 0.9729\n",
            "Epoch 150/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.2459 - val_acc: 0.9729\n",
            "Epoch 151/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.2481 - val_acc: 0.9731\n",
            "Epoch 152/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 0.2330 - val_acc: 0.9730\n",
            "Epoch 153/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.2411 - val_acc: 0.9730\n",
            "Epoch 154/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.2418 - val_acc: 0.9730\n",
            "Epoch 155/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2456 - val_acc: 0.9731\n",
            "Epoch 156/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2476 - val_acc: 0.9729\n",
            "Epoch 157/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2479 - val_acc: 0.9730\n",
            "Epoch 158/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2489 - val_acc: 0.9730\n",
            "Epoch 159/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0035 - acc: 0.9986 - val_loss: 0.2494 - val_acc: 0.9731\n",
            "Epoch 160/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.2398 - val_acc: 0.9730\n",
            "Epoch 161/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.2447 - val_acc: 0.9732\n",
            "Epoch 162/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.2457 - val_acc: 0.9729\n",
            "Epoch 163/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2446 - val_acc: 0.9729\n",
            "Epoch 164/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.2417 - val_acc: 0.9730\n",
            "Epoch 165/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0062 - acc: 0.9978 - val_loss: 0.1643 - val_acc: 0.9730\n",
            "Epoch 166/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0082 - acc: 0.9973 - val_loss: 0.1761 - val_acc: 0.9731\n",
            "Epoch 167/500\n",
            "536/536 [==============================] - 55s 102ms/step - loss: 0.0053 - acc: 0.9979 - val_loss: 0.1868 - val_acc: 0.9730\n",
            "Epoch 168/500\n",
            "536/536 [==============================] - 54s 102ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.2137 - val_acc: 0.9730\n",
            "Epoch 169/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.2205 - val_acc: 0.9732\n",
            "Epoch 170/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.2243 - val_acc: 0.9731\n",
            "Epoch 171/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.2318 - val_acc: 0.9730\n",
            "Epoch 172/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 0.2322 - val_acc: 0.9732\n",
            "Epoch 173/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 0.2371 - val_acc: 0.9731\n",
            "Epoch 174/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 0.2375 - val_acc: 0.9730\n",
            "Epoch 175/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 0.2374 - val_acc: 0.9732\n",
            "Epoch 176/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.2443 - val_acc: 0.9731\n",
            "Epoch 177/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.2441 - val_acc: 0.9731\n",
            "Epoch 178/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 0.2460 - val_acc: 0.9730\n",
            "Epoch 179/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.2461 - val_acc: 0.9731\n",
            "Epoch 180/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0030 - acc: 0.9988 - val_loss: 0.2500 - val_acc: 0.9731\n",
            "Epoch 181/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 0.2479 - val_acc: 0.9730\n",
            "Epoch 182/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.2500 - val_acc: 0.9733\n",
            "Epoch 183/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 0.2580 - val_acc: 0.9731\n",
            "Epoch 184/500\n",
            "536/536 [==============================] - 54s 101ms/step - loss: 0.0027 - acc: 0.9989 - val_loss: 0.2591 - val_acc: 0.9732\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e6af6cf4ae42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Train the Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_whitened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodelCheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_whitened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Dumping Weights to file...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0m_serialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_serialize_model\u001b[0;34m(model, f, include_optimizer)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mlayer_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, attr, val)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                 \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, args, val)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNLIMITED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfspace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wEhNL6jKq96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}