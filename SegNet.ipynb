{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyPZ3SnZHNLZ",
        "colab_type": "text"
      },
      "source": [
        "# Shubham Shrama\n",
        "## IIT BOMBAY\n",
        "In this repository,  we'll impliment SegNet network on the cell dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0QDMvsEGPd1",
        "colab_type": "code",
        "outputId": "4d2eb38a-0c35-4845-fb60-238ad0c8ebdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#As my data is on my drive in Cell_data folder, we'll have to mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive') \n",
        "PATH = 'gdrive/My Drive/Data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47uyObfSGCaa",
        "colab_type": "code",
        "outputId": "9dc1c8ac-6c7b-4339-c987-ebbe63cf63fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "\n",
        "@author: Shubham sharma\n",
        "'''\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "from keras.optimizers import RMSprop,SGD, adam\n",
        "from keras.layers import Input,Conv2D,Conv2DTranspose,MaxPool2D,\\\n",
        "UpSampling2D,BatchNormalization,Activation,MaxPooling2D,Permute,Flatten,Reshape,ZeroPadding2D,Dropout\n",
        "from keras import callbacks \n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#from GeneralizationPOC.NetPixel import TransferLearning_19_Heart,TransferLearning_19_Test_rbc,U_Net\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "np.random.seed(1337) # for reproducibility\n",
        "import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "#from FirstPackage.Network import net_1\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
        "set_session(tf.Session(config=config))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chdhCpT0Balq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#############################################################################\n",
        "def TransferLearning_19_Test_functional(width=512, height=512, depth=3, weightsPath=None,classes=2):\n",
        "    \n",
        "    keras.backend.set_image_data_format('channels_first')\n",
        "    data_shape = width*height\n",
        "    classes = 2;\n",
        "    psize = (2,2)\n",
        "    pdsize = (1,1)    \n",
        "    ksize = 3\n",
        "    dout = 0.25\n",
        "    \n",
        "    bn_flag = True\n",
        "    do_flag = True\n",
        "            \n",
        "    inp = Input((depth,height,width))\n",
        "    x = ZeroPadding2D((1,1))(inp) #1    #This layer can add rows and columns of zeros at the top, bottom, left and right side of an image tensor.\n",
        "    \n",
        "    #layer 1-9 in keras\n",
        "    c = (Conv2D(64, 3, 3))(x)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(64, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    mx = (MaxPooling2D((2,2), strides=(2,2)))(bn)#10\n",
        "    do = (Dropout(rate=dout))(mx,training=do_flag)\n",
        "    \n",
        "    #layer 10-19 in keras\n",
        "    do = (ZeroPadding2D((1,1)))(do)\n",
        "    c = (Conv2D(128, 3, 3))(do)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(128, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    mx = (MaxPooling2D((2,2), strides=(2,2)))(bn)#10\n",
        "    do = (Dropout(rate=dout))(mx,training=do_flag)\n",
        "\n",
        "    #layer 20-37 in keras\n",
        "    do = (ZeroPadding2D((1,1)))(do)\n",
        "    c = (Conv2D(256, 3, 3))(do)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(256, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(256, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(256, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    mx = (MaxPooling2D((2,2), strides=(2,2)))(bn)#10\n",
        "    do = (Dropout(rate=dout))(mx,training=do_flag)\n",
        "\n",
        "    #layer 38-55 in keras\n",
        "    do = (ZeroPadding2D((1,1)))(do)\n",
        "    c = (Conv2D(512, 3, 3))(do)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    mx = (MaxPooling2D((2,2), strides=(2,2)))(bn)#10\n",
        "    do = (Dropout(rate=dout))(mx,training=do_flag)\n",
        "\n",
        "    #layer 56-71 in keras\n",
        "    do = (ZeroPadding2D((1,1)))(do)\n",
        "    c = (Conv2D(512, 3, 3))(do)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)#4, 5\n",
        "    bn = (ZeroPadding2D((1,1)))(bn)\n",
        "    c = (Conv2D(512, 3, 3))(bn)\n",
        "    a = (Activation(\"relu\"))(c)#3\n",
        "    bn = (BatchNormalization(axis = 1))(a,training=bn_flag)\n",
        "    \n",
        "    #===================================== Decoder Layers =============================================\n",
        "    \n",
        "    #layer 72-74 in keras\n",
        "    upsize = (2,2)\n",
        "    # Conv Layer\n",
        "    #bn = (Dropout(rate=dout))(bn,training=do_flag)\n",
        "    up = (UpSampling2D(size=upsize))(bn)#70\n",
        "    up = (ZeroPadding2D(padding=pdsize))(up)\n",
        "    c = (Conv2D(64,ksize,ksize,border_mode='valid'))(up)#71\n",
        "            \n",
        "    #layer 75-77 in keras            \n",
        "    upsize = (2,2)\n",
        "    # Conv Layer\n",
        "    #c = (Dropout(rate=dout))(c,training=do_flag)\n",
        "    up = (UpSampling2D(size=upsize))(c)#72\n",
        "    up = (ZeroPadding2D(padding=pdsize))(up)\n",
        "    c = (Conv2D(64,ksize,ksize,border_mode='valid'))(up)#73    \n",
        "    #layer 78-80 in keras\n",
        "    upsize = (2,2)\n",
        "    # Conv Layer\n",
        "    #c = (Dropout(rate=dout))(c,training=do_flag)\n",
        "    up = (UpSampling2D(size=upsize))(c)#72\n",
        "    up = (ZeroPadding2D(padding=pdsize))(up)\n",
        "    c = (Conv2D(32,ksize,ksize,border_mode='valid'))(up)#73    \n",
        "        \n",
        "    #layer 81-83 in keras        \n",
        "    upsize = (2,2)\n",
        "    # Conv Layer\n",
        "    up = (UpSampling2D(size=upsize))(c)#72\n",
        "    up = (ZeroPadding2D(padding=pdsize))(up)\n",
        "    c = (Conv2D(16,ksize,ksize,border_mode='valid'))(up)#73\n",
        "            \n",
        "    #curr_depth, curr_height, curr_width = net.layers[-1].output_shape[1:]    \n",
        "    #print(str(curr_width)+\" \"+str(curr_height)+\" \"+str(curr_depth))\n",
        "    #================================== Final Layer with softmax =======================================\n",
        "    \n",
        "    #layer 84-88 in keras\n",
        "    do = (Dropout(rate=dout))(c)\n",
        "    c  = (Conv2D(classes,1,1,border_mode='valid'))(do)#78 \n",
        "    data_shape = width * height\n",
        "    \n",
        "    #curr_depth, curr_height, curr_width = net.layers[-1].output_shape[1:]\n",
        "    r = (Reshape((classes,data_shape),input_shape=(classes,height,width)))(c)\n",
        "    p = (Permute((2,1)))(r)            #Permutes the dimensions of the input according to a given pattern.\n",
        "    s = (Activation(\"softmax\"))(p) #79 80\n",
        "    \n",
        "    net = Model(inputs=inp,output=[s])\n",
        "    #=========================== If a pre trained model is supplied =====================================\n",
        "    if weightsPath is not None:\n",
        "        net.load_weights(weightsPath)\n",
        "        \n",
        "    print(net.summary())\n",
        "            \n",
        "    return net\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXnXFKfyGPgN",
        "colab_type": "code",
        "outputId": "930af53f-d3e3-443e-8c31-5ac79e5462b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4285
        }
      },
      "source": [
        "#Define Directories to read numpy arrays\n",
        "train_data_path = 'gdrive/My Drive/Data/'\n",
        "label_path = 'gdrive/My Drive/Data/'\n",
        "\n",
        "#Define Model Path\n",
        "preTrained = './Models/liver_bg_U_Net_BN.h5'\n",
        "path2save = 'gdrive/My Drive/Cell_data/Cell.h5'\n",
        "\n",
        "#Define Image Size and Number of classes\n",
        "imheight = 512\n",
        "imwidth = 512\n",
        "imdepth = 3\n",
        "data_shape = imheight*imwidth\n",
        "classes = 2\n",
        "\n",
        "#Load numpy arrays from the directory\n",
        "train_data = np.load(train_data_path+'data_cell.npy')\n",
        "# train_data = np.transpose(train_data,[0,2,3,1])\n",
        "#train_data = train_data.astype(\"float32\")\n",
        "\n",
        "label = np.load(label_path+'label_cell.npy')\n",
        "\n",
        "\n",
        "label = np.reshape(label,(len(label),data_shape,classes))\n",
        "print(\"Here\")\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data, label, test_size=0.20)\n",
        "\n",
        "#Create Network Graph in the Memory\n",
        "net = TransferLearning_19_Test_functional(weightsPath='gdrive/My Drive/Cell_data/Cell.h5_0012-0.9846.h5')\n",
        "# net = net_1()\n",
        "\n",
        "\n",
        "\n",
        "#net = U_Net(imwidth,imheight,imdepth,preTrained)\n",
        " \n",
        "print (\"Compiling Model...\")\n",
        "#Store the network weights whenever loss is minimum than previous epoch\n",
        "modelCheck = callbacks.ModelCheckpoint(path2save+'_{epoch:04d}-{acc:.4f}.h5', monitor='acc', mode='auto')\n",
        "# modelCheck = callbacks.ModelCheckpoint(path2save, monitor='acc', verbose=0, save_best_only=True, mode='auto')\n",
        " \n",
        "opt = adam(lr=1e-4)\n",
        "#Set the compiler parameter for the training\n",
        "net.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"],sample_weight_mode='auto')\n",
        "print (\"Training the Model...\")\n",
        " \n",
        "#Train the Network\n",
        "net.fit(x_train, y_train, batch_size = 1, epochs= 100, verbose=1,callbacks= [modelCheck],validation_data=(x_test, y_test))\n",
        "print (\"Dumping Weights to file...\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:149: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:153: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:161: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:165: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:173: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:177: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:181: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:185: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:193: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:197: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:201: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:205: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:213: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:217: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:221: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:225: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:237: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:245: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:252: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:259: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:267: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(2, (1, 1), padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:275: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=[<tf.Tenso...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Compiling Model...\n",
            "Training the Model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 536 samples, validate on 134 samples\n",
            "Epoch 1/100\n",
            "536/536 [==============================] - 112s 209ms/step - loss: 0.0476 - acc: 0.9811 - val_loss: 0.0432 - val_acc: 0.9826\n",
            "Epoch 2/100\n",
            "536/536 [==============================] - 106s 198ms/step - loss: 0.0425 - acc: 0.9827 - val_loss: 0.0438 - val_acc: 0.9828\n",
            "Epoch 3/100\n",
            "536/536 [==============================] - 107s 199ms/step - loss: 0.0372 - acc: 0.9847 - val_loss: 0.0408 - val_acc: 0.9845\n",
            "Epoch 4/100\n",
            "536/536 [==============================] - 107s 199ms/step - loss: 0.0333 - acc: 0.9862 - val_loss: 0.0410 - val_acc: 0.9840\n",
            "Epoch 5/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0328 - acc: 0.9865 - val_loss: 0.0417 - val_acc: 0.9837\n",
            "Epoch 6/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0304 - acc: 0.9874 - val_loss: 0.0443 - val_acc: 0.9831\n",
            "Epoch 7/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0296 - acc: 0.9877 - val_loss: 0.0470 - val_acc: 0.9828\n",
            "Epoch 8/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0273 - acc: 0.9887 - val_loss: 0.0467 - val_acc: 0.9829\n",
            "Epoch 9/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0281 - acc: 0.9883 - val_loss: 0.0476 - val_acc: 0.9820\n",
            "Epoch 10/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0253 - acc: 0.9895 - val_loss: 0.0479 - val_acc: 0.9831\n",
            "Epoch 11/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0239 - acc: 0.9901 - val_loss: 0.0520 - val_acc: 0.9830\n",
            "Epoch 12/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0230 - acc: 0.9904 - val_loss: 0.0513 - val_acc: 0.9822\n",
            "Epoch 13/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0225 - acc: 0.9906 - val_loss: 0.0509 - val_acc: 0.9827\n",
            "Epoch 14/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0231 - acc: 0.9905 - val_loss: 0.0507 - val_acc: 0.9819\n",
            "Epoch 15/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0216 - acc: 0.9910 - val_loss: 0.0571 - val_acc: 0.9821\n",
            "Epoch 16/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0219 - acc: 0.9909 - val_loss: 0.0549 - val_acc: 0.9824\n",
            "Epoch 17/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0201 - acc: 0.9916 - val_loss: 0.0559 - val_acc: 0.9825\n",
            "Epoch 18/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0199 - acc: 0.9917 - val_loss: 0.0553 - val_acc: 0.9821\n",
            "Epoch 19/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0192 - acc: 0.9920 - val_loss: 0.0600 - val_acc: 0.9823\n",
            "Epoch 20/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0187 - acc: 0.9922 - val_loss: 0.0715 - val_acc: 0.9795\n",
            "Epoch 21/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0183 - acc: 0.9924 - val_loss: 0.0620 - val_acc: 0.9823\n",
            "Epoch 22/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0183 - acc: 0.9924 - val_loss: 0.0633 - val_acc: 0.9813\n",
            "Epoch 23/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0174 - acc: 0.9927 - val_loss: 0.0591 - val_acc: 0.9823\n",
            "Epoch 24/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0171 - acc: 0.9929 - val_loss: 0.0658 - val_acc: 0.9815\n",
            "Epoch 25/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0168 - acc: 0.9930 - val_loss: 0.0686 - val_acc: 0.9808\n",
            "Epoch 26/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0163 - acc: 0.9933 - val_loss: 0.0643 - val_acc: 0.9816\n",
            "Epoch 27/100\n",
            "536/536 [==============================] - 107s 199ms/step - loss: 0.0154 - acc: 0.9936 - val_loss: 0.0651 - val_acc: 0.9817\n",
            "Epoch 28/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0161 - acc: 0.9933 - val_loss: 0.0674 - val_acc: 0.9816\n",
            "Epoch 29/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0160 - acc: 0.9934 - val_loss: 0.0645 - val_acc: 0.9819\n",
            "Epoch 30/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0150 - acc: 0.9938 - val_loss: 0.0700 - val_acc: 0.9815\n",
            "Epoch 31/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0158 - acc: 0.9935 - val_loss: 0.0672 - val_acc: 0.9816\n",
            "Epoch 32/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0143 - acc: 0.9941 - val_loss: 0.0658 - val_acc: 0.9816\n",
            "Epoch 33/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0139 - acc: 0.9943 - val_loss: 0.0729 - val_acc: 0.9816\n",
            "Epoch 34/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0142 - acc: 0.9942 - val_loss: 0.0675 - val_acc: 0.9814\n",
            "Epoch 35/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0142 - acc: 0.9941 - val_loss: 0.0726 - val_acc: 0.9811\n",
            "Epoch 36/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0133 - acc: 0.9945 - val_loss: 0.0722 - val_acc: 0.9817\n",
            "Epoch 37/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0130 - acc: 0.9947 - val_loss: 0.0736 - val_acc: 0.9812\n",
            "Epoch 38/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0134 - acc: 0.9945 - val_loss: 0.0786 - val_acc: 0.9814\n",
            "Epoch 39/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0130 - acc: 0.9947 - val_loss: 0.0761 - val_acc: 0.9810\n",
            "Epoch 40/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0128 - acc: 0.9947 - val_loss: 0.0795 - val_acc: 0.9809\n",
            "Epoch 41/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0126 - acc: 0.9948 - val_loss: 0.0820 - val_acc: 0.9807\n",
            "Epoch 42/100\n",
            "536/536 [==============================] - 107s 199ms/step - loss: 0.0121 - acc: 0.9950 - val_loss: 0.0786 - val_acc: 0.9813\n",
            "Epoch 43/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0116 - acc: 0.9953 - val_loss: 0.0816 - val_acc: 0.9811\n",
            "Epoch 44/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0116 - acc: 0.9952 - val_loss: 0.0819 - val_acc: 0.9807\n",
            "Epoch 45/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0119 - acc: 0.9951 - val_loss: 0.0851 - val_acc: 0.9813\n",
            "Epoch 46/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0112 - acc: 0.9954 - val_loss: 0.0802 - val_acc: 0.9808\n",
            "Epoch 47/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0106 - acc: 0.9956 - val_loss: 0.0892 - val_acc: 0.9811\n",
            "Epoch 48/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0106 - acc: 0.9957 - val_loss: 0.0773 - val_acc: 0.9808\n",
            "Epoch 49/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0107 - acc: 0.9956 - val_loss: 0.0879 - val_acc: 0.9811\n",
            "Epoch 50/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0107 - acc: 0.9957 - val_loss: 0.0883 - val_acc: 0.9810\n",
            "Epoch 51/100\n",
            "536/536 [==============================] - 107s 201ms/step - loss: 0.0107 - acc: 0.9956 - val_loss: 0.0829 - val_acc: 0.9809\n",
            "Epoch 52/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.0851 - val_acc: 0.9808\n",
            "Epoch 53/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.0894 - val_acc: 0.9808\n",
            "Epoch 54/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0107 - acc: 0.9956 - val_loss: 0.0928 - val_acc: 0.9803\n",
            "Epoch 55/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0101 - acc: 0.9959 - val_loss: 0.0899 - val_acc: 0.9808\n",
            "Epoch 56/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0096 - acc: 0.9961 - val_loss: 0.0935 - val_acc: 0.9807\n",
            "Epoch 57/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0094 - acc: 0.9962 - val_loss: 0.0846 - val_acc: 0.9805\n",
            "Epoch 58/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0090 - acc: 0.9964 - val_loss: 0.0961 - val_acc: 0.9809\n",
            "Epoch 59/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0092 - acc: 0.9963 - val_loss: 0.0981 - val_acc: 0.9808\n",
            "Epoch 60/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0094 - acc: 0.9962 - val_loss: 0.0943 - val_acc: 0.9808\n",
            "Epoch 61/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0093 - acc: 0.9962 - val_loss: 0.0981 - val_acc: 0.9809\n",
            "Epoch 62/100\n",
            "536/536 [==============================] - 107s 201ms/step - loss: 0.0089 - acc: 0.9964 - val_loss: 0.1010 - val_acc: 0.9808\n",
            "Epoch 63/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0090 - acc: 0.9964 - val_loss: 0.0991 - val_acc: 0.9806\n",
            "Epoch 64/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0086 - acc: 0.9965 - val_loss: 0.0982 - val_acc: 0.9807\n",
            "Epoch 65/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0084 - acc: 0.9966 - val_loss: 0.1074 - val_acc: 0.9801\n",
            "Epoch 66/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.1018 - val_acc: 0.9808\n",
            "Epoch 67/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.1049 - val_acc: 0.9808\n",
            "Epoch 68/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0083 - acc: 0.9967 - val_loss: 0.1060 - val_acc: 0.9807\n",
            "Epoch 69/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.0971 - val_acc: 0.9804\n",
            "Epoch 70/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0081 - acc: 0.9967 - val_loss: 0.1070 - val_acc: 0.9804\n",
            "Epoch 71/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.1069 - val_acc: 0.9805\n",
            "Epoch 72/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 0.0995 - val_acc: 0.9807\n",
            "Epoch 73/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0080 - acc: 0.9968 - val_loss: 0.1063 - val_acc: 0.9805\n",
            "Epoch 74/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.1057 - val_acc: 0.9806\n",
            "Epoch 75/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0077 - acc: 0.9969 - val_loss: 0.1089 - val_acc: 0.9805\n",
            "Epoch 76/100\n",
            "536/536 [==============================] - 108s 201ms/step - loss: 0.0073 - acc: 0.9970 - val_loss: 0.1086 - val_acc: 0.9803\n",
            "Epoch 77/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0074 - acc: 0.9970 - val_loss: 0.1091 - val_acc: 0.9803\n",
            "Epoch 78/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0074 - acc: 0.9970 - val_loss: 0.1090 - val_acc: 0.9804\n",
            "Epoch 79/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0072 - acc: 0.9971 - val_loss: 0.1102 - val_acc: 0.9805\n",
            "Epoch 80/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 0.1148 - val_acc: 0.9802\n",
            "Epoch 81/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0071 - acc: 0.9972 - val_loss: 0.1137 - val_acc: 0.9801\n",
            "Epoch 82/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 0.1095 - val_acc: 0.9805\n",
            "Epoch 83/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0068 - acc: 0.9973 - val_loss: 0.1153 - val_acc: 0.9801\n",
            "Epoch 84/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0066 - acc: 0.9973 - val_loss: 0.1138 - val_acc: 0.9804\n",
            "Epoch 85/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0067 - acc: 0.9973 - val_loss: 0.1057 - val_acc: 0.9799\n",
            "Epoch 86/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0070 - acc: 0.9972 - val_loss: 0.1094 - val_acc: 0.9796\n",
            "Epoch 87/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 0.1230 - val_acc: 0.9802\n",
            "Epoch 88/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0065 - acc: 0.9974 - val_loss: 0.1233 - val_acc: 0.9802\n",
            "Epoch 89/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0066 - acc: 0.9974 - val_loss: 0.1144 - val_acc: 0.9802\n",
            "Epoch 90/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 0.1180 - val_acc: 0.9804\n",
            "Epoch 91/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.1213 - val_acc: 0.9802\n",
            "Epoch 92/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.1241 - val_acc: 0.9800\n",
            "Epoch 93/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.1200 - val_acc: 0.9801\n",
            "Epoch 94/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0058 - acc: 0.9977 - val_loss: 0.1252 - val_acc: 0.9802\n",
            "Epoch 95/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.1236 - val_acc: 0.9803\n",
            "Epoch 96/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0062 - acc: 0.9976 - val_loss: 0.1155 - val_acc: 0.9798\n",
            "Epoch 97/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.1246 - val_acc: 0.9803\n",
            "Epoch 98/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0057 - acc: 0.9977 - val_loss: 0.1221 - val_acc: 0.9803\n",
            "Epoch 99/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0056 - acc: 0.9978 - val_loss: 0.1281 - val_acc: 0.9801\n",
            "Epoch 100/100\n",
            "536/536 [==============================] - 107s 200ms/step - loss: 0.0054 - acc: 0.9979 - val_loss: 0.1323 - val_acc: 0.9799\n",
            "Dumping Weights to file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcEmh2jo9kAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "segmented_imgs = net.predict(x_test)\n",
        "segmented_imgs_train=net.predict(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYN1BnKFe5uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# n = 10\n",
        "# plt.figure(figsize=(20, 4))\n",
        "# for i in range(n):\n",
        "#     # display original\n",
        "#     ax = plt.subplot(2, n, i+1)\n",
        "#     plt.imshow(x_test[i].reshape(512, 512,3))\n",
        "#     ax.get_xaxis().set_visible(False)\n",
        "#     ax.get_yaxis().set_visible(False)\n",
        "\n",
        "#     # display reconstruction\n",
        "#     ax = plt.subplot(2, n, i + n +1)\n",
        "#     plt.imshow(segmented_imgs[i].reshape(512, 512,2))\n",
        "#     plt.gray()\n",
        "#     ax.get_xaxis().set_visible(False)\n",
        "#     ax.get_yaxis().set_visible(False)\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAxhn3GY-BNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def change_binary_to_gray(arr):\n",
        "#   z=[]\n",
        "#   for i in range(len(arr)):\n",
        "#     if arr[i][0]>=arr[i][1]:\n",
        "#       z.append(0)\n",
        "#     else:\n",
        "#       z.append(1)\n",
        "#   z=np.array(z)\n",
        "#   z=np.reshape(z,(512,512))\n",
        "#   return z"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}